from xml.etree.ElementTree import parse, Element, SubElement, Comment, tostring, fromstring
import xml.etree.ElementTree as ET
import urllib.request
import datetime
from pathlib import Path
from urllib.request import urlopen, Request
from urllib.error import HTTPError, URLError
import socket
import logging


def seve_opml(nodes, output_path):
    generated_on = str(datetime.datetime.now())

    root = Element('opml')
    root.set('version', '1.0')

    root.append(Comment('Generated by ElementTree_csv_to_xml.py for PyMOTW'))

    head = SubElement(root, 'head')
    title = SubElement(head, 'title')
    title.text = 'My Podcasts'
    dc = SubElement(head, 'dateCreated')
    dc.text = generated_on
    dm = SubElement(head, 'dateModified')
    dm.text = generated_on

    body = SubElement(root, 'body')
    current_group = None
    for node in nodes:
        name = node.attrib.get('text')
        url = node.attrib.get('xmlUrl')
        htmlUrl = node.attrib.get('htmlUrl')
        if name and url:
            print('  %s' % name)
            print('    %s' % url)
            print('    %s' % htmlUrl)
            SubElement(current_group, 'outline',
                       {'text': name,
                        'xmlUrl': url,
                        'htmlUrl': htmlUrl,
                        })
        else:
            current_group = SubElement(body, 'outline', {'text': name})
            print(name)

    et = ET.ElementTree()
    et._setroot(root)
    et.write(output_path)


def read_opml(file_path):


    my_file = Path(file_path)
    if my_file.is_file() is False:
        print("파일이 없습니다.")
        return -1

    with open(file_path, 'rt') as f:
        tree = parse(f)

    nodes = []
    for node in tree.iter('outline'):
        nodes.append(node)

    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) ' 
                      'AppleWebKit/537.11 (KHTML, like Gecko) '
                      'Chrome/23.0.1271.64 Safari/537.11',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',
        'Accept-Encoding': 'none',
        'Accept-Language': 'en-US,en;q=0.8',
        'Connection': 'keep-alive'}

    for node in nodes:
        name = node.attrib.get('text')
        url = node.attrib.get('xmlUrl')
        if name and url:
            rss_data = None

            try:
                rss_request = Request(url=url, headers=headers)
                rss_data = urlopen(rss_request).read()

                if rss_data is not None and type(rss_data) == str() and len(rss_data) > 0:
                    reddit_root = fromstring(rss_data)
                    item = reddit_root.findall('channel/item')

                    reddit_feed = []
                    for entry in item:
                        pubDate = entry.findtext('pubDate')
                        reddit_feed.append([pubDate])

                    if len(reddit_feed) > 0 and len(reddit_feed[0]) > 0:
                        first_date = reddit_feed[0][0]

                        if first_date.find("2018") >= 0 or first_date.find("2019") >= 0:
                            continue
                        else:
                            nodes.remove(node)
                    else:
                        nodes.remove(node)
                else:
                    nodes.remove(node)

            except HTTPError as error:
                logging.error('Data of %s not retrieved because %s\nURL: %s', name, error, url)
                nodes.remove(node)
            except URLError as error:
                nodes.remove(node)
                if isinstance(error.reason, socket.timeout):
                    logging.error('socket timed out - URL %s', url)
                else:
                    logging.error('some other error happened - URL %s', url)

    return nodes


def main():
    nodes = read_opml("./feed.opml")
    if type(nodes) == list():
        seve_opml(nodes, "output.opml")


if __name__ == "__main__":
    main()
